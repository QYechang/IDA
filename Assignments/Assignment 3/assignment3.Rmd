---
title: "Assignment 3 PCA"
author: "Nicholas Jacob and Yechang Qi"
date: "2024-09-05"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(corrplot)
library(ggfortify)
library(MASS)
library(caret)
library(magrittr)
library(dplyr)
library(HSAUR2)
library(outliers)
library(ggbiplot)
```

## Glass Data

1.a.i We'll load the data directly from our machine.  We've added the names from the info included in the zip file with the data.

```{r, include =TRUE}
names <- c('Id', 'RI','Na','Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type')
Glass <- read.csv('glass.data',header = FALSE)
names(Glass)<- names
Glass$Type = as.character(Glass$Type)
head(Glass)
```

Let's double check that there isn't a duplicate since there was a warning about a possible dup.
```{r}
any(!duplicated(Glass))
```

Does not appear to be any duplicates in this dataset.  

Now we'll create the correlation matrix.  We should remove the 'Id' and 'Type' from the data before we do the correlation.  Originally we saw strong correlation between Id and Type only because the data was ordered by type

```{r}
corMat <- cor(Glass[,2:10])
corMat
```
No ridiculously strong correlations but RI-CA is about 0.8 so it is fairly strong.

1.a.ii. Compute the eigen values and vectors of this matrix.

```{r}
eMat <- eigen(corMat)
eMat
```

As expected for the correlation matrix, all values are positive.  Eigen vectors don't show me much yet.

1.a.iii.  Next we'll do the principle component decomposition

```{r}
pMat <- prcomp(Glass[,2:10],scale = TRUE)
pMat
```
We see RI and Ca well represented in the first PCA vector.

1.a.iv. These are different.  The correlation is a scaled version of the covariance matrix so essentially, these are built off of similar matrices up to a rescaling.  If before building the correlation e-values, we did a $z$-score rescaling they would be exactly the same.  Instead as we will see in the next section, they both form the same ortho-normal basis.

1.a.v.  Show that these vectors are orthogonal to each other.

First, I needed to see how to access the vectors
```{r}
eMat$vectors[,1] #
```

We did not find a function for doing the inner product but since the formula is 
$$
x\cdot y = \sum_{i=1}^nx_iy_i
$$
We simply asked R to do that with base packages.
```{r}
sum(pMat$rotation[,1]*eMat$vectors[,1])
```

We see that this one is indeed 1 as expected.  Let's build a matrix of all $9^2$ possible combinations.  Yes some are repeated but that does not increase runtime significantly for this data.
```{r}
idMat <- data.frame(matrix(nrow = 9, ncol = 9)) #initialize matrix
for(i in 1:9){
  for (j in 1:9){
    idMat[i,j] <- sum(pMat$rotation[,i]*eMat$vectors[,j]) #add data to matrix
  }
}
idMat #print matrix
```

We this is essentially an identity matrix but it has the overflow errors.  Let's force those to be zero.

```{r}
idMat[abs(idMat)<0.00000001] = 0 #force really small values to be zero so you can inspect
idMat
```

We might be worried about the negative ones, but we recall that the inner product is negative one if the vectors point opposite each other but otherwise the same direction.  So we do see that these two matrix have a orthonormal column space (up to a negative direction).

1.b.i.

```{r}
heatmap(corMat, main = "Heatmap of Correlation for Glass Data", scale = "column", Rowv = NA, Colv = NA)
```

I tried for a while to get a scale for the colors on this graph and failed to find it.  I am going to use the package suggestion of `corrplot` and of course it works great and has the scale that I wanted in the defaults.  Yeah open source for the win!
```{r}
corrplot(corMat)
```

Yes there are two solutions here but I find it important to show that sometimes the base package just doesn't have what you want.

1.b.ii.

To look at the PCA, I'll start with the overloaded `plot` from the S3 method included with the `prcomp` function.  The scree plot follows
```{r}
plot(pMat, main = "Scree Plot for PCA of Glass", xlab = "PCA Components")
```

I don't quite see the hockey stick I expected.  I also expected this to be normalized to one but here we see variances above 1 which is odd and not what I recalled in scree plots.

```{r}
autoplot(pMat, data = Glass, 
         color = "Type", 
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE)+
  scale_color_manual(breaks = c("1","2","3","5","6","7"),
                    values =c("red","blue","green","violet","yellow","black"))
```

We like like this graphic a lot for showing off the different categories and the amount of the components in the PCA directions.

1.b.iii.

The first two components of the PCA provide the direction in the 9 dimensional space in which the most variance is explained.  We see that 27.9\% and 22.7\% of the variance are explained in these two directions.  We also see that iron (fe) and silicon (si) contribute negligibly to the first two PCAs.  This first direction of the PCA is dominated by Al, Ca, and RI. The second component is dominated by Mg and Ba.

1.b.iv.

There is some value to this PCA but only covering about 50\% of the variation has some drawbacks.  The lack of the hockeystick in the scree plot also makes me hesitant to use this PCA.  Besides all this, the dataset is relatively small dimensional at 9 dimensions.  Reducing to two or three dimensions would allow for slightly faster computations but overall, I do not think it is worth it here only getting 50\% of the variation.  No, I would not suggest using the PCA in this case.

1.c.i.
First, I will mean center the data.

```{r}
preproc.param <- Glass[2:11] %>% preProcess(method = c("center","scale"))

transformed <- preproc.param %>% predict(Glass[2:11])


```

Now we will apply the LDA transformation on the transformed data.
```{r}
ldaData <- lda(Type~., transformed)
ldaData
```

1.c.ii.  We see that the first LD1 projection picks about 81.45\% of the differences of the means.  We see that Na, Al, Si and Ca are the most important variables in our determining which class the class belongs to.
We can use the following formula to make our predictions with the LDA1.

$$
0.94*RI+1.94*Na + 1.06*Mg + 1.66*Al + 1.90*Si + 1.02*K +1.43* Ca +1.15*Ba - .05*Fe
$$

1.c.iii.

We make our predictions here
```{r}
values <- predict(ldaData, transformed)
#values$x[,1]
#ldahist(data = values$x[,1], g = Glass$Type)
```
This is the overloaded plot for the lda.  I am always curious to see these for new methods I use.
```{r}
plot(ldaData, col = as.numeric(Glass$Type))
```
A confusion table to check out the results
```{r}
table(Orignial = Glass$Type, Predicted = values$class)
```
We see in our confusion table that we did well but not perfect.  Let's see what it actually did

```{r}
mean(values$class == Glass$Type)
```

Correct about two thirds of the time, much better than guessing!

A scatterplot of the LD1 and LD2 with coloring.

```{r}
lda.data  <- cbind(transformed,predict(ldaData)$x)
ggplot(lda.data, aes(LD1,LD2)) +
  geom_point(aes(color = Type))
```

We do not see perfect separation and understand why our results are not a bit stronger.  I could not get the command `ldahist` to compile on my machine.  Instead I went ahead and used ggplot to create something equivalent.

```{r}
ggplot(lda.data, aes(LD1)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```
We see some separation in the first LDA but below in the second,  we see almost none.

```{r}
ggplot(lda.data, aes(LD2)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```

2a.

Load the data and run on outlier test on the data.
```{r 2a1}
# Load the data
data("heptathlon", package = "HSAUR2")

# Apply Grubbs' test to each event
outlier_tests <- apply(heptathlon[, 1:7], 2, function(x) grubbs.test(x))

# Print out the results
outlier_tests
```
According to this test,  the competitor Launa (PNG) is identified as an outlier in multiple events: 

1. Hurdles: Highest value 16.42 is an outlier, with a p-value of 0.000436 (very strong evidence).
2. High jump: Lowest value 1.5 is an outlier, with a p-value of 0.0001698 (very strong evidence).
3. Long jump: Lowest value 4.88 is an outlier, with a p-value of 0.04594 (moderate evidence).
4. 800m run: Highest value 163.43 is an outlier, with a p-value of 0.001808 (strong evidence).

Remove Launa (PNG) from the dataset as she is consistently identified as an outlier across multiple events.

```{r 2a2}
heptathlon_cleaned <- heptathlon[-which(rownames(heptathlon) == "Launa (PNG)"), ]
```

2b.

```{r 2b1}
# Identify the columns for the running events
running_events <- c("hurdles", "run200m", "run800m")

# Apply the transformation: subtract each value from the maximum value in its column
heptathlon_cleaned[running_events] <- apply(
  heptathlon_cleaned[running_events],2,
  function(x) max(x) - x)
```

By transforming all the running events by subtracting each athlete's time from the maximum recorded time in the event, we can more easily compare performance across events where higher values consistently represent better outcomes.

```{r 2b2}
# Check the transformed data for running events
head(heptathlon_cleaned[running_events])
```

2c.

```{r 2c}
# Perform PCA on the 7 event results
Hpca <- prcomp(heptathlon_cleaned[, 1:7], scale. = TRUE)

# View a summary of the PCA results
summary(Hpca)

# Check the rotation matrix (loadings)
Hpca$rotation

# Check the PCA scores (principal components)
head(Hpca$x)
```

2d.
```{r 2d}
# Visualize the first two principal components using ggbiplot
ggbiplot(Hpca, obs.scale = 1, var.scale = 1, 
         circle = TRUE) +
  ggtitle("PCA Biplot of Heptathlon Events") +
  theme_minimal() +
  theme(legend.position = "bottom")

```
PC1 (explains 61.8% of the variance): Separates competitors based on running events (800m, 200m, hurdles). Athletes with higher PC1 scores performed better in running events, while those with lower PC1 scores did better in field events (high jump, javelin).

PC2 (explains 12.8% of the variance): Mainly reflects performance in the high jump. Athletes with higher PC2 scores excelled in high jump.

Event Correlation:

1. Running events (800m, 200m, hurdles) are positively related, as their arrows point in similar directions.

2. Field events (high jump, javelin) are negatively related to running events, meaning athletes who performed well in running events tended to perform worse in field events.

Competitors: Athletes farther from the center of the plot showed more extreme performances (either very good or bad in certain events), while those closer to the center had more balanced performances across all events.

2e.
```{r 2e}
# Assuming the heptathlon score is stored in a column called 'score' in the dataset
# Replace 'score' with the actual column name if different
heptathlon_scores <- heptathlon_cleaned$score

# Plot heptathlon score vs. PC1 projections
plot(Hpca$x[,1], heptathlon_scores, 
     xlab = "PC1 Projections", 
     ylab = "Heptathlon Score", 
     main = "Heptathlon Score vs. PC1 Projections",
     pch = 19, col = "blue")

# Add a linear regression line to see the relationship
abline(lm(heptathlon_scores ~ Hpca$x[,1]), col = "red")
```

The plot shows a negative linear relationship between the heptathlon score and PC1 projections. Since PC1 primarily reflects performance in running events, higher PC1 values indicate stronger performance in these events. The negative slope of the regression line suggests that competitors with lower PC1 projections tend to have higher overall heptathlon scores. This indicates that excelling in running events plays a significant role in achieving a higher overall score. Conversely, competitors with higher PC1 values tend to have lower total scores, meaning weaker performance in running events is associated with a lower heptathlon score.

In summary, the plot highlights that performance in running events is a key factor in determining a competitor's overall heptathlon success.









3.

Load up the data and get it going...
```{r}
housingData = read.csv("housingData-1.csv")
hd <- housingData %>%
  select_if(is.numeric) %>%
  dplyr::mutate(age = YrSold - YearBuilt,
              ageSinceRemodel = YrSold - YearRemodAdd,
              ageofGarage = ifelse(is.na(GarageYrBlt), age, YrSold - GarageYrBlt)) %>%
  dplyr::select(!c(Id,MSSubClass, LotFrontage, GarageYrBlt,
                MiscVal, YrSold , MoSold, YearBuilt,
                YearRemodAdd, MasVnrArea))
```

Next we will preform the PCA of the scaled data.
```{r}
hd.pca <- prcomp(hd,scale = TRUE)
plot(hd.pca)
```

The plot shows that the first few principal components explain a large portion of the variance.

PC1: Focuses on property size and overall quality features like the number of floors, the size of the garage, and the overall quality of the house.

PC2: Likely reflects the age and remodel aspects of the house, as it is influenced by variables such as age and ageSinceRemodel.

Other components: Capture smaller variations, but still significant, related to more nuanced characteristics such as lot size and specific areas like basement finishes or features like porches.

Let's look at the vectors and see what contributes the most to the principle components.

```{r}
autoplot(hd.pca, data = hd, 
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE)
```

That is way too busy of a graph to glean much info.  Let's look to the correlations and see what we can get from that.

```{r}
cor(hd) %>% corrplot()
```
