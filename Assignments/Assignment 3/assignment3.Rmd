---
title: "Assignment 3 PCA"
author: "Nicholas Jacob and Yechang Qi"
date: "2024-09-05"
output: pdf_document
---

```{r setup}
knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(corrplot)
library(ggfortify)
library(MASS)
library(caret)
library(magrittr)
```

## Glass Data

1.a.i I'll load the data directly from my machine.  I wanted to load it directly from the web but seems as though those links don't exist anymore.  I've added the names from the info included in the zip file with the data.

```{r}
names <- c('Id', 'RI','Na','Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe', 'Type')
Glass <- read.csv('glass.data',header = FALSE)
names(Glass)<- names
Glass$Type = as.character(Glass$Type)
head(Glass)
```

Let's double check that there isn't a duplicate since there was a warning about a possible dup.
```{r}
any(!duplicated(Glass))
```

Does not appear to be any duplicates in this dataset.  Now we'll create the correlation matrix.  We should remove the 'Id' and 'Type' from the data before we do the correlation.  Originally we saw strong correlation between Id and Type only because the data was organized by type

```{r}
corMat <- cor(Glass[,2:10])
corMat
```
1.a.ii. Compute the eigen values and vectors of this matrix.

```{r}
eMat <- eigen(corMat)
eMat
```

As expected for the correlation matrix, all values are positive.  Eigen vectors don't show me much yet.

1.a.iii.  Next we'll do the principle component decomposition

```{r}
pMat <- prcomp(Glass[,2:10],scale = TRUE)
pMat
```

1.a.iv. These are different.  The correlation is a scaled version of the covariance matrix so essentially, these are built off of similar matrices up to a rescaling.  If before building the correlation e-values, we did a $z$-score rescaling they would be exactly the same.  Instead as we will see in the next section, they both form the same ortho-normal basis.

1.a.v.  Show that these vectors are orthogonal to each other.

First, I needed to see how to access the vectors
```{r}
eMat$vectors[,1] #
```

We did not find a function for doing the inner product but since the formula is 
$$
x\cdot y = \sum_{i=1}^nx_iy_i
$$
We simply asked R to do that with base packages.
```{r}
sum(pMat$rotation[,1]*eMat$vectors[,1])
```

We see that this one is indeed 1 as expected.  Let's build a matrix of all $9^2$ possible combinations.  Yes some are repeated but that does not increase runtime significantly for this data.
```{r}
idMat <- data.frame(matrix(nrow = 9, ncol = 9)) #initialize matrix
for(i in 1:9){
  for (j in 1:9){
    idMat[i,j] <- sum(pMat$rotation[,i]*eMat$vectors[,j]) #add data to matrix
  }
}
idMat #print matrix
```

We this is essentially an identity matrix but it has the overflow errors.  Let's force those to be zero.

```{r}
idMat[abs(idMat)<0.00000001] = 0 #force really small values to be zero so you can inspect
idMat
```

We might be worried about the negative ones, but we recall that the inner product is negative one if the vectors point opposite each other but otherwise the same direction.  So we do see that these two matrix have a orthonormal column space.

1.b.i.

```{r}
heatmap(corMat, main = "Heatmap of Correlation for Glass Data", scale = "column", Rowv = NA, Colv = NA)
```

I tried for a while to get a scale for the colors on this graph and failed to find it.  I am going to use the package suggestion of `corrplot` and of course it works great and has the scale that I wanted in the defaults.  Yeah open source for the win!
```{r}
corrplot(corMat)
```

Yes there are two solutions here but I find it important to show that sometimes the base package just doesn't have what you want.

1.b.ii.

To look at the PCA, I'll start with the overloaded `plot` from the S3 method included with the `prcomp` function.  The scree plot follows
```{r}
plot(pMat, main = "Scree Plot for PCA of Glass", xlab = "PCA Components")
```

I don't quite see the hockey stick I expected.  I also expected this to be normalized to one but here we see variances above 1 which is odd and not what I recalled in scree plots.

```{r}
autoplot(pMat, data = Glass, 
         color = "Type", 
         loadings = TRUE, 
         loadings.colour = 'blue',
         loadings.label = TRUE)+
  scale_color_manual(breaks = c("1","2","3","5","6","7"),
                    values =c("red","blue","green","violet","yellow","black"))
```
1.b.iii.

The first two components of the PCA provide the direction in the 9 dimensional space in which the most variance is explained.  We see that 27.9\% and 22.7\% of the variance are explained in these two directions.  We also see that iron (fe) and silicon (si) contribute negligibly to the first two PCAs.  This first direction of the PCA is dominated by Al, Ca, and RI. The second component is dominated by Mg and Ba.

1.b.iv.

There is some value to this PCA but only covering about 50\% of the variation has some drawbacks.  The lack of the hockeystick in the scree plot also makes me hesitant to use this PCA.  Besides all this, the dataset is relatively small dimensional at 9 dimensions.  Reducing to two or three dimensions would allow for slightly faster computations but overall, I do not think it is worth it here only getting 50\% of the variation.  No, I would not suggest using the PCA in this case.

1.c.i.
First, I will mean center the data.

```{r}
preproc.param <- Glass[2:11] %>% preProcess(method = c("center","scale"))

transformed <- preproc.param %>% predict(Glass[2:11])


```

Now we will apply the LDA transformation on the transformed data.
```{r}
ldaData <- lda(Type~., transformed)
ldaData
```

1.c.ii.  We see that the first LD1 projection picks about 81.45\% of the differences of the means.  We see that Na, Al, Si and Ca are the most important variables in our determining which class the class belongs to.
We can use the following formula to make our predictions with the LDA1.

$$
0.94*RI+1.94*Na + 1.06*Mg + 1.66*Al + 1.90*Si + 1.02*K +1.43* Ca +1.15*Ba - .05*Fe
$$

1.c.iii.

We make our predictions here
```{r}
values <- predict(ldaData, transformed)
#values$x[,1]
#ldahist(data = values$x[,1], g = Glass$Type)
```
This is the overloaded plot for the lda.  I am always curious to see these for new methods I use.
```{r}
plot(ldaData, col = as.numeric(Glass$Type))
```
A confusion table to check out the results
```{r}
table(Orignial = Glass$Type, Predicted = values$class)
```
We see in our confusion table that we did well but not perfect.  Let's see what it actually did

```{r}
mean(values$class == Glass$Type)
```

Correct about two thirds of the time, much better than guessing!

A scatterplot of the LD1 and LD2 with coloring.

```{r}
lda.data  <- cbind(transformed,predict(ldaData)$x)
ggplot(lda.data, aes(LD1,LD2)) +
  geom_point(aes(color = Type))
```

We do not see perfect separation and understand why our results are not a bit stronger.  I could not get the command `ldahist` to compile on my machine.  Instead I went ahead and used ggplot to create something equivalent.

```{r}
ggplot(lda.data, aes(LD1)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```
We see some separation in the first LDA but below in the second,  we see almost none.

```{r}
ggplot(lda.data, aes(LD2)) +
  geom_histogram() +
  facet_grid(rows = "Type")
```
