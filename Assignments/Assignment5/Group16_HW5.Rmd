---
title: "Assignment 5 Predictions"
author: "Nicholas Jacob and Yechang Qi"
date: "2024-09-05"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
#library(corrplot)
#library(ggfortify)
#library(MASS)
library(caret)
library(magrittr)
library(dplyr)
#library(HSAUR2)
#library(outliers)
#library(ggbiplot)
#library(GGally)
#library(gridExtra)
#library(mice)
library(forcats)
library(knitr)
library(car)
library(pls)
```
# Predicting House Prices

1a.

We start by reading in the data and doing some cleaning.  With the large presence of `Na`s in the data, we convert `Na` into a factor for factor data and...

```{r}
housingData = read.csv("housingData-1.csv")

housingFactor <- housingData %>% 
  dplyr::select(where(is.character))%>%
  mutate_all(factor) %>%
  mutate_all(fct_na_value_to_level) #Needed to convert NA's to a level to make the regression have enough data to go.

housingNumeric <- housingData %>% 
  dplyr::select(where(is.numeric)) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), 0))

housingData <- cbind(housingFactor, housingNumeric)


OLM <- lm(log(SalePrice) ~ ., data= housingData )

summary(OLM)
```
There are lots of variables here.  Let's see if we can reduce the number of variables.  The following code starts with the complete model from above and adds or removes features based on $AIC$, attempting to minimize this value.  We have hidden the results because they were very long.
```{r, results = 'hide'}
fit1 <- stepAIC(OLM, direction = "both")
```
Below is the formula for the best found in the above process.
```{r}
fit1$call$formula
fit1$coefficients
```
Now that we have found this linear fit, we run a 5 fold cross validation to examine the 

```{r}
fitControl <- trainControl(method="cv", number=5
                           )

fit <- train(fit1$call$formula,
             data=housingData,
             method="lm",
             trControl=fitControl)

fit

fit$results

```
```{r}
summary(fit)
```
```{r}
fit <- lm(fit1$call$formula,
             data=housingData)
AIC(fit)
BIC(fit)
```

```{r}
plot(fit, which = 1)
```
Looking at the diagnostics here, we see a decent spread and while there are a few identified outliers, it is not too bad.  Diagnostically, we are looking for
1.Points above and Below the line
2. no patterns
3. no change in variation (no cones)
4. normal distribution

While we think we have this, the hypothesis test shows otherwise.
```{r}
ncvTest(fit)
```
Oh, this is a problem!  We are able to reject the null hypothesis here, we have evidence to suggest that the error variance changes with the fitted values.  Let's keep looking at some of the other results we can get.

```{r}
hist(fit$residuals, breaks = 20)
```
This looks fairly normal but with a longer tail on the negative side than the positive.  Maybe qq-plot will show me something?
```{r}
plot(fit, which = 2)
```
Here we see our deviance from normal on that low end once again.  Next we look at the standardized residuals versus the predicted.

```{r}
scatterplot(fit$fitted.values,rstandard(fit))
```
Here we see plenty of outer space values (over 4) and some really strange (over 3)

Next we look for values with high leverage.
```{r}

plot(index = 1:1000,hatvalues(fit))
abline(h = 0.2)
```
We see lots with of points with high leverage (above the 0.2 line)
```{r}
sum(hatvalues(fit)>0.2)
```
10\% of our data has extreme leverage. 

Let's test for outliers.

```{r}
outlierTest(fit)
```
We find two outliers that are influential.  Lastly we look at Cook's Distance.
```{r}
plot(cooks.distance(fit),rstudent(fit))
```
Not terrible but still some rather extreme points.

```{r}
influencePlot(fit)
```
This one is nice for seeing these issues.

```{r}
vif(fit)
```
There is definately some colinearity since the average is way over one.

One more fancy visualization with `ggplot` and some residuals.  Not super useful but it was the first I put together and it looks pretty.


```{r residualViz}
housingData$Predicted <- predict(fit)
housingData$logSalePrice <- log(housingData$SalePrice)
ggplot(housingData, aes(x = YearBuilt, y = logSalePrice, color = Exterior1st)) +
  geom_segment(aes(xend = YearBuilt, yend = Predicted), alpha = .2) +
  geom_point()+
  geom_point(aes(y = Predicted), shape = 1)+
  facet_wrap(Neighborhood~.) 
  
```
1.b.

```{r}
model.pls <- pls::plsr(log(SalePrice) ~ .-logSalePrice - Predicted, 16, data = housingData, #method = "oscorespls", 
                       validation = "CV")
summary(model.pls)
```
```{r}
model.pls$validation$PRESS
```

```{r}
plot(model.pls)

plot(model.pls, ncomp = 1, asp = 1, line = TRUE)
plot(model.pls, ncomp = 2, asp = 1, line = TRUE)
plot(model.pls, ncomp = 3, asp = 1, line = TRUE)
plot(model.pls, ncomp = 4, asp = 1, line = TRUE)
plot(model.pls, ncomp = 16, asp = 1, line = TRUE)
```

```{r}
plot(model.pls, plottype = "scores", comps = 1:4)
```
```{r}

model <- caret::train(log(SalePrice) ~ .-logSalePrice - Predicted,
               data = housingData,
               method = "lasso",
              # scale = TRUE,
               trControl = fitControl#,
#               tuneLength = 10
               )

model
```



```{r, include = FALSE}
names(housingData)
```
