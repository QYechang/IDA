---
title: "Assignment 5 Predictions"
author: "Nicholas Jacob and Yechang Qi"
date: "2024-09-05"
output: pdf_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
library(ggplot2)
#library(corrplot)
#library(ggfortify)
#library(MASS)
library(caret)
library(magrittr)
library(dplyr)
#library(HSAUR2)
#library(outliers)
#library(ggbiplot)
#library(GGally)
#library(gridExtra)
#library(mice)
library(forcats)
library(knitr)
library(car)
library(pls)
```
# Predicting House Prices

1a.

We start by reading in the data and doing some cleaning.  With the large presence of `Na`s in the data, we convert `Na` into a factor for factor data and...

```{r}
housingData = read.csv("housingData-1.csv")

housingFactor <- housingData %>% 
  dplyr::select(where(is.character))%>%
  mutate_all(factor) %>%
  mutate_all(fct_na_value_to_level) #Needed to convert NA's to a level to make the regression have enough data to go.

housingNumeric <- housingData %>% 
  dplyr::select(where(is.numeric)) %>%
  mutate_if(is.numeric, ~replace(., is.na(.), 0))

housingData <- cbind(housingFactor, housingNumeric)


OLM <- lm(log(SalePrice) ~ ., data= housingData )

summary(OLM)
```
There are lots of variables here.  Let's see if we can reduce the number of variables.  The following code starts with the complete model from above and adds or removes features based on $AIC$, attempting to minimize this value.  We have hidden the results because they were very long.
```{r, results = 'hide'}
fit1 <- stepAIC(OLM, direction = "both")
```
Below is the formula for the best found in the above process.
```{r}
fit1$call$formula
fit1$coefficients
```
Now that we have found this linear fit, we run a 5 fold cross validation to examine the 

```{r}
fitControl <- trainControl(method="cv", number=5)

fit <- train(fit1$call$formula,
             data=housingData,
             method="lm",
             trControl=fitControl)

fit

fit$results

```
```{r}
summary(fit)
```
```{r}
fit <- lm(fit1$call$formula,
             data=housingData)
AIC(fit)
BIC(fit)
vif(fit)
```



Next we attempt to visualize some residuals.  


```{r residualViz}
housingData$Predicted <- predict(fit)
housingData$logSalePrice <- log(housingData$SalePrice)
ggplot(housingData, aes(x = YearBuilt, y = logSalePrice, color = Exterior1st)) +
  geom_segment(aes(xend = YearBuilt, yend = Predicted), alpha = .2) +
  geom_point()+
  geom_point(aes(y = Predicted), shape = 1)+
  facet_wrap(Neighborhood~.) 
  
```
1.b.

```{r}
model.pls <- pls::plsr(log(SalePrice) ~ .-logSalePrice - Predicted, 16, data = housingData, #method = "oscorespls", 
                       validation = "CV")
summary(model.pls)
```
```{r}
model.pls$validation$PRESS
```

```{r}
plot(model.pls)

plot(model.pls, ncomp = 1, asp = 1, line = TRUE)
plot(model.pls, ncomp = 2, asp = 1, line = TRUE)
plot(model.pls, ncomp = 3, asp = 1, line = TRUE)
plot(model.pls, ncomp = 4, asp = 1, line = TRUE)
plot(model.pls, ncomp = 16, asp = 1, line = TRUE)
```

```{r}
plot(model.pls, plottype = "scores", comps = 1:4)
```
```{r}
model <- train(log(SalePrice) ~ .-logSalePrice - Predicted,
               data = housingData,
               method = "pls",
               scale = TRUE,
               trControl = fitControl,
               tuneLength = 5
               )
```



```{r, include = FALSE}
names(housingData)
```
