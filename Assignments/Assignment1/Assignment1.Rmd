---
title: "Assignment 1"
author: "Nicholas Jacob"
date: "2024-08-19"
output: pdf_document
---

```{r setup}
knitr::opts_chunk$set(echo = TRUE)
library(plyr)
```

## Using R:  Vectors


Using `c` to combine the values, we see that $x$ is a vector.
```{r 1a}
x<- c(3,12,6,-5,0,8,15,1,-10,7)
is.vector(x)
```
To create the new vector $y$ as a sequence from the min of $x$ to the max of $x$, we do the following:
```{r 1b}
y <-seq(min(x),max(x), length.out = 10)
y
```
I was not familiar with the `length.out` command but found it in the Help package to see that it would restrict the output to that many elements.

We compute the desired stats next

```{r 1c}
#consider changing this one with some tidy code
sum(x)
sum(y)
mean(x)
mean(y)
sd(x)
sd(y)
var(x)
var(y)
mad(x)
mad(y)
quantile(x,1/4)
quantile(y,1/4)
quantile(x,3/4)
quantile(y,3/4)
quantile(x,1/5)
quantile(y,1/5)
quantile(x,3/5)
quantile(y,3/5)
quantile(x,2/5)
quantile(y,2/5)
quantile(x,4/5)
quantile(y,4/5)
```
To do sampling with replacement we do the following

```{r 1d}
sample(x,7,TRUE)
```
The `TRUE` gives the replacement.  Some instances do see repeated vales.

Next we do the `t.test`
```{r}
t.test(x,y)
```
We fail to reject the null hypothesis here.  There is no evidence to suggest that the mean values are different.

Next we explore the `order` function.

```{r 1g}
order(x)
```
We see this gives the order of the elements of $x$, indexing at 1 as the lowest value.  To sort $x$ we could do the following.
```{r 1g2}
sort(x)
```

We could also use the order function as follows:
```{r}
x[order(x)]
```

Inside the [] we are giving the index of the value we want.  So this will return the values in the proper order.  Lastly we will preform the paired t.test.

```{r}
t.test(sort(x),y,paired = TRUE)
```

The result here is still not significant (for p =0.05) but is much closer than in the non-paired data.  I am actually quite surprised at that result but since $y$ is build off of $x$ and now they are both sequential I could see why they might be statistically equivalent on average.

A logical test for negativity is simply
```{r}
x>0
```

Since this gives the Boolean, we can use that as the index for $x$ and overwrite $x$
```{r}
x <- x[x>0]
x
```

## Using R:  Some Missing Values

```{r}
col1 <- c(1,2,3,NA,5)
col2 <- c(4,5,6,89,101)
col3 <- c(45,NA,66,121,201)
col4 <- c(14,NA,13,NA,27)
X <- rbind (col1,col2,col3,col4)

X


```

So we see $X$ has NA in three rows.  We can find the NAs with the following
```{r}
is.na(X)
```
To get to which rows have the NAs, we sum across the booleans and ask that the sum in that row is larger than 0.  Then we use the rownames command to give out those rows names that do have some NAs.
```{r}
rownames(X)[rowSums(is.na(X))>0]
```

For the next piece, we define $y$

```{r}
y <- c(3,12,99,99,7,99,21)
y
```

We will find the 99s with this peice of code
```{r}
y == 99
```

We set that to the NA value with this which overwrites y values.
```{r}
y[y==99] = NA

y
```

I count the NA values with a sum of the booleans

```{r}
sum(is.na(y))
```

##Using R:  IDE

Here I have read the data in.  I utilize the head command to display the first 6 rows.
```{r}
college = read.csv('college.csv')
head(college)
```

Next, I change the rownames to the university name and delete that column.
```{r}
rownames (college) <- college [,1]
college <- college [,-1]
head(college)
```

Next we examine some stats on the data
```{r}
summary(college)
```

I am not familiar with the `pairs` command but here goes
```{r}
pairs(college[,2:10])
```

That is a nice graphic although a bit too small for my tastes.  I hope it compiles correctly in the pdf...

Next I'll create the boxplot for out of state tution vs the public or private.
```{r}
boxplot(Outstate ~ Private, data = college, main = "Out Of State Tuition by College Type", ylab = "Out of State Tuition")
```
This looks fine although I do prefer `ggplot2`.

Next I comment the code as requested
```{r}
Elite <- rep ("No", nrow(college )) #This creates a vector that full of No that is the same width as the dataframe
Elite [college$Top10perc >50] <- "Yes" #this changes some of the nos to yes if the top10 is more than 50%
Elite <- as.factor (Elite) #this casts the vector as a factor vector.  This is useful in that Elite now has levels
college <- data.frame(college ,Elite) #this adds the column to the original dataframe and saves it

summary(Elite)
```

It appears that there are 78 elite universities.  Let's explore tutions with this new factor
```{r}
boxplot(Outstate ~ Elite, data = college, main = "Out Of State Tuition by Elite Institutions", ylab = "Out of State Tuition")
```

Next we look at a few histograms with differing number of bins.

```{r}
par(mfrow=c(2,2))
hist(college[,'Enroll'], main = "Default", xlab = "Enrollment")
hist(college[,'Enroll'], main = "10 bins",breaks = 10, xlab = "Enrollment")
hist(college[,'Enroll'], main = "20 bins",breaks = 20, xlab = "Enrollment")
hist(college[,'Enroll'], main = "50 bins",breaks = 50, xlab = "Enrollment")
```

I don't see much difference between the default, 10 nor 20.  The 50 does look a bit different.

Again just to try it once more
```{r}
par(mfrow=c(2,2))
hist(college[,'Room.Board'], main = "Default", xlab = "Room and Board Cost")
hist(college[,'Room.Board'], main = "10 bins",breaks = 10, xlab = "Room and Board Cost")
hist(college[,'Room.Board'], main = "20 bins",breaks = 20, xlab = "Room and Board Cost")
hist(college[,'Room.Board'], main = "50 bins",breaks = 50, xlab = "Room and Board Cost")
```

It kind of looks like more than 10 breaks  Maybe the default overrides that option if you set it too low... 

## Using R:  Manipulating Data in Data Frames

First, I'll load some data directly from a package.  This `baseball` data comes from the `plyr` package loaded earlier.

```{r}
head(baseball)
```
Lots of baseball data!

```{r}
baseball[baseball$year<1954,'sf'] = 0 #set all sf before 1954 to 0
baseball[is.na(baseball$hbp),'hbp'] = 0 #set all null values for hit by pitch to 0
baseball <- baseball[baseball$ab>=50,]


```

Now that the data is clean, we will apply the obp formula of 
$$
obp = \frac{h+bb+hbp}{ab+bb+hpb+sf}
$$

```{r}
baseball <- mutate(baseball, obp = (h+bb+hbp)/(ab+bb+hbp+sf))

head(baseball)
```

Now that we have that info added, let's find the top five players for obp of all time.

```{r}
arrange(baseball, -obp)[1:5,c('year','id','obp')] #I get the top records with 1:5, restrict on to the columns asked for and sort via arrange and the negative gives the max rather than min
```
We see here Barry Bonds (from the 'roids era twice), Ted Williams(a year he hit .400), John McGraw (a player I was not familiar with though he did have a season with my home team Cardinals in 1900) and the babe himself Babe Ruth.

##Using R:  `aggregate()` Function

I am going to grab the `quakes` dataset.
```{r}
head(quakes)
```

Next we will examine magnitude versus depth with a scatter plot.

```{r}
plot(quakes$depth,quakes$mag, xlab = 'Depth', ylab = 'Magnitude', main = 'Scatter Plot of Depth vs Magnitude')
```
Next we will aggregate the data to look at the average depth for each of the magnitude levels
```{r}
quakeAvgDepth = aggregate(quakes$depth, list(mag = quakes$mag), mean)
```
Not too bad when you follow the example in the help menu.

Next I rename the dataframe to have useful column names and print it to see the nice output.

```{r}
colnames(quakeAvgDepth) = c('mag','meanDepth')
head(quakeAvgDepth)
```

Now we plot again to see if there is a relationship in the aggregate

```{r}
plot(quakeAvgDepth$mag,quakeAvgDepth$meanDepth, xlab = 'Magnitude', ylab = 'Average Depth',main = 'Scatter of Aggregated Magnitude vs Mean Depth')
```

There clearly appears to be a relationship here.  It was not as obvious in the full data case but the relationship appears in the aggregate.  I do question a bit of this methodology though.  We are aggregating a continuous variable that has been truncated to two decimals.  Richter scale (magnitude) is a famous example of a logarithmic scale so small rounding errors are amplified in varying degrees as you increase the scale.  While yes, I believe there is a relationship, I'd be worried about generalizing too far based on this data.
